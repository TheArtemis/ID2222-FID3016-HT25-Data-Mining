% Very simple template for lab reports. Most common packages are already included.
\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc} % Change according to your file encoding
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float} % provides [H] placement to pin floats
\usepackage{placeins} % provides \FloatBarrier to prevent floats from passing barriers


%opening
\title{Report 2: Itemsets}
\author{Group 150: Lorenzo Deflorian, Riccardo Fragale}
\date{\today{}}

\begin{document}

\maketitle


\section{Introduction}

This homework was about discovering frequent itemsets and generating association rules.
These two problems are important especially in the field of sales transaction databases,
since companies want to discover and understand the logic behind customers' behavior.
The procedure is quite straightforward and we tested it on a dataset provided by the teacher that includes 100,000 generated transactions (baskets) of hashed items.
We decided not to use Apache Spark since our algorithms performed efficiently enough.

\section{Our Methods}
We needed to develop solutions to address the following two problems:
\begin{itemize}
    \item How to find frequent itemsets with support at least \textit{s}
    \item How to generate association rules with confidence at least \textit{c} from the itemsets found
\end{itemize}

\subsection{A-Priori Algorithm Implementation}
We implemented the A-Priori algorithm (standard level-wise frequent itemset mining procedure) in the \textbf{apriori} class to solve the first problem.
The constructor stores baskets and support threshold \textit{s}, computing an absolute threshold = \textit{s} $\times$ len(baskets).
The \textit{run} method controls the iterative passes \textit{k} = 1, 2, \ldots and stops when no new frequent itemsets are found.
We call \textit{run\_pass(k)} to orchestrate each single pass: it generates singletons for \textit{k}=1, and for higher \textit{k} it performs candidate generation, counting, and filtering.

For efficiency, we count itemsets basket-by-basket rather than materializing all combinations at once, which minimizes peak memory usage.
The function \textit{has\_frequent\_subsets(candidate, prev\_frequent)} implements Apriori's pruning strategy: all (\textit{k}-1)-subsets must be frequent for a candidate to be retained.
This pruning greatly reduces the number of candidates stored in memory.

We implemented \textit{frequent\_by\_size} to store persistent frequent itemsets grouped by size, releasing ephemeral candidate data after each pass.
The \textit{frequent\_items\_table} method flattens all discovered frequent itemsets for reporting in tests.
Overall, our algorithm follows classic Apriori heuristics to discover frequent itemsets while maintaining controlled memory usage.

\subsection{Association Rule Generation}
After testing the Apriori implementation, we implemented association rule generation using the \textbf{association\_rules} class.
The \textit{AssociationRule} class serves as a simple data holder containing: antecedent (frozenset), consequent (int), support, confidence, and interest metrics.
We initialize the \textit{AssociationRuleGenerator} with frequent itemsets, baskets, and threshold parameters.
Internally, we maintain an \textit{ItemsetAnalyzer} instance to compute support, confidence, and interest metrics on demand.

Our \textit{generate()} method iterates through all frequent itemsets and skips singletons (only itemsets of size $\geq$ 2 produce rules).
For each itemset, we call \textit{process\_itemset()}, which builds candidate rules by using each item as a consequent and the remaining items as the antecedent.
The \textit{generate\_rule()} method constructs the antecedent frozenset and uses \textit{ItemsetAnalyzer} to compute the support, confidence, and interest values.
It returns an \textit{AssociationRule} object populated with these metrics.

Finally, \textit{filter\_rule()} applies minimum support, confidence, and interest thresholds to accept or reject each rule.
Accepted rules are appended to the final list returned by \textit{generate()}.
Our generator does not create large intermediate data structures; it simply iterates through existing frequent itemsets and computes metrics as needed.
The computational cost is mainly determined by \textit{ItemsetAnalyzer} calls, which either scan baskets or reuse precomputed summaries.
The output is a flat list of \textit{AssociationRule} objects, ready for sorting or grouping by the caller.

\subsection{Testing}
We implemented comprehensive tests on the provided dataset to validate our implementation and measure algorithm performance.


\section{Results}
We ran two main tests: \textbf{test\_apriori} and \textbf{test\_rule\_generator}.

\subsection{Apriori Results}
The Apriori test produced the following results:
\begin{itemize}
    \item 381 frequent itemsets
    \item Itemsets by size: [375, 6]
\end{itemize}
The algorithm completed in 0.460 seconds.

\subsection{Association Rule Results}
In the second test, which includes the Apriori procedure followed by rule generation as described above,
we discovered 12 association rules:
\begin{verbatim}
INFO   AssociationRule({227} -> 390, s=0.0105, c=0.5770, i=0.5502)
INFO   AssociationRule({390} -> 227, s=0.0105, c=0.3907, i=0.3725)
INFO   AssociationRule({346} -> 217, s=0.0134, c=0.3850, i=0.3313)
INFO   AssociationRule({390} -> 722, s=0.0104, c=0.3881, i=0.3296)
INFO   AssociationRule({217} -> 346, s=0.0134, c=0.2486, i=0.2139)
INFO   AssociationRule({682} -> 368, s=0.0119, c=0.2887, i=0.2104)
INFO   AssociationRule({789} -> 829, s=0.0119, c=0.2771, i=0.2090)
INFO   AssociationRule({722} -> 390, s=0.0104, c=0.1783, i=0.1514)
INFO   AssociationRule({829} -> 789, s=0.0119, c=0.1753, i=0.1322)
INFO   AssociationRule({368} -> 682, s=0.0119, c=0.1524, i=0.1111)
INFO   AssociationRule({829} -> 368, s=0.0119, c=0.1753, i=0.0971)
INFO   AssociationRule({368} -> 829, s=0.0119, c=0.1525, i=0.0844
\end{verbatim}

The average confidence of the generated rules is \textbf{0.2824}, with an average interest of \textbf{0.2328}.
The total execution time was \textbf{0.483} seconds, indicating that the Apriori algorithm is the more computationally expensive step compared to rule generation.

\section{Conclusion}
This assignment helped us thoroughly understand the steps required to identify association rules.
We gained insight into the importance of these techniques, their potential economic impact, and their widespread use, particularly regarding scalability concerns.

In future work, we would like to test our implementation on larger datasets and possibly on data streams
to verify whether our expectations regarding performance and scalability hold in practice.
\end{document}
